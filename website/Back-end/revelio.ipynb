{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../Facial-Landmarks-Detector/')\n",
    "sys.path.append('../../Revelio-LipsMovement/')\n",
    "sys.path.append('../../Dynamic-texture-analysis-for-detecting-fake-faces-in-video-sequences/')\n",
    "sys.path.append('../../Modules/dft/lib/')\n",
    "sys.path.append('../../SBI2/')\n",
    "sys.path.append('../../FaceDetection/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from revelio import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../Facial-Landmarks-Detector/')\n",
    "sys.path.append('../../Revelio-LipsMovement/')\n",
    "sys.path.append('../../Dynamic-texture-analysis-for-detecting-fake-faces-in-video-sequences/')\n",
    "sys.path.append('../../Modules/dft/lib/')\n",
    "sys.path.append('../../SBI2/')\n",
    "sys.path.append('../../FaceDetection/')\n",
    "\n",
    "from revel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import dlib\n",
    "from landmarks_detector import *\n",
    "from LipMovementClassifier import *\n",
    "from FDA_Model import *\n",
    "from DynamicTexture import *\n",
    "from sbi_inference import *\n",
    "from detect_face import *\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_frames(videoCapture):\n",
    "    #read frames\n",
    "    grayFrames = []\n",
    "    coloredFrames = []\n",
    "    while True:\n",
    "        ret, frame = videoCapture.read()\n",
    "        if ret:\n",
    "            #frame to gray scale\n",
    "            coloredFrames.append(frame)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            grayFrames.append(frame)\n",
    "        else:\n",
    "            break\n",
    "    return grayFrames, coloredFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_face_detector():\n",
    "    face_detector = joblib.load('../../FaceDetection/hFeatures6/faceDetector2.joblib')\n",
    "    all_classifiers = face_detector.classifier.strong_classifiers[0].weak_classifiers\n",
    "    face_detector.classifier.strong_classifiers[0].weak_classifiers = all_classifiers[:80]\n",
    "    face_detector.classifier.strong_classifiers[0].Î¸ = np.sum(face_detector.classifier.strong_classifiers[0].alphas)/2\n",
    "    face_detector.stride = 10\n",
    "    face_detector.scale_dist = 1.25\n",
    "    return face_detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face_region(face_detector, grayFrame):\n",
    "    original_size = grayFrame.shape\n",
    "    maxdim = 320\n",
    "    if grayFrame.shape[0] > maxdim or grayFrame.shape[1] > maxdim:\n",
    "        if grayFrame.shape[0] > grayFrame.shape[1]:\n",
    "            grayFrame = cv2.resize(grayFrame, (int(maxdim * grayFrame.shape[1] / grayFrame.shape[0]), maxdim))\n",
    "        else:\n",
    "            grayFrame = cv2.resize(grayFrame, (maxdim, int(maxdim * grayFrame.shape[0] / grayFrame.shape[1])))\n",
    "    _, region, _, time= face_detector.find_face(grayFrame, min_size=50)\n",
    "\n",
    "    #rescale region\n",
    "    x1,y1,x2,y2 = region\n",
    "\n",
    "    x1 = int(x1 * original_size[1] / grayFrame.shape[1])\n",
    "    x2 = int(x2 * original_size[1] / grayFrame.shape[1])\n",
    "    y1 = int(y1 * original_size[0] / grayFrame.shape[0])\n",
    "    y2 = int(y2 * original_size[0] / grayFrame.shape[0])\n",
    "\n",
    "    region = (x1,y1,x2,y2)\n",
    "    return region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_frames(face_detector, grayFrames, coloredFrames):\n",
    "    #detector = dlib.get_frontal_face_detector()\n",
    "    faceFrames = []\n",
    "    coloredFaceFrames = []\n",
    "    for i in range(len(grayFrames)):\n",
    "        frame = grayFrames[i]\n",
    "        coloredFrame = coloredFrames[i]\n",
    "        #face = detector(frame)[0]\n",
    "        #x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()\n",
    "        x1,y1,x2,y2 = detect_face_region(face_detector, frame)\n",
    "\n",
    "        faceFrames.append(frame[y1:y2, x1:x2])\n",
    "        coloredFaceFrames.append(coloredFrame[y1:y2, x1:x2])\n",
    "    \n",
    "    #resize face frames to the minimum frace size\n",
    "    minSize = min([face.shape[0] for face in faceFrames])\n",
    "    faceFrames = [cv2.resize(face, (minSize, minSize)) for face in faceFrames]\n",
    "    coloredFaceFrames = [cv2.resize(face, (minSize, minSize)) for face in coloredFaceFrames]\n",
    "\n",
    "    return np.array(faceFrames), np.array(coloredFaceFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbi_analysis(sbiModel, seqLength, coloredFaceFrames):\n",
    "    #split coloredFaceFrames into sequences of 32 frames\n",
    "    splitFrames = np.array([coloredFaceFrames[i:i+seqLength] for i in range(0, (len(coloredFaceFrames)//seqLength)*seqLength, seqLength)])\n",
    "    sbi_predictions = []\n",
    "    for sequence in splitFrames:\n",
    "        result = sbiModel.infer(sequence, num_frames=seqLength)\n",
    "        sbi_predictions.append(result)\n",
    "    return sbi_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeLipsMovementModel():\n",
    "    LipsModelsLocations = '../../Revelio-LipsMovement/trainedmodels/'\n",
    "    LipModelsPaths = [LipsModelsLocations + 'deepfakes/lips_movements_classifer.pth', LipsModelsLocations + 'face2face/lips_movements_classifer.pth', LipsModelsLocations + 'faceswap/lips_movements_classifer.pth', LipsModelsLocations + 'neuraltextures/lips_movements_classifer.pth']\n",
    "    lipsMovementModel = LipMovementClassifier(isPredictor=True, predictionMSTCNModelPaths=LipModelsPaths, featureExtractorModelPath='../../Revelio-LipsMovement/trainedmodels/resnet_feature_extractor.pth', mstcnConfigFilePath='../../Revelio-LipsMovement/models/configs/mstcn.json')\n",
    "    return lipsMovementModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_analysis(allLipsPredictions, fdaPredictions, dynamicTexturePredictionsBinary,dynamicTexturePredictionsMulti, sbiPredictions):\n",
    "    resultmap = {\n",
    "        'LipDF': (1-allLipsPredictions[0]).flatten().tolist(),\n",
    "        'LipDFAvg': 1-np.average(allLipsPredictions[0]),\n",
    "        'LipF2F': (1-allLipsPredictions[1]).flatten().tolist(),\n",
    "        'LipF2FAvg': 1-np.average(allLipsPredictions[1]),\n",
    "        'LipFS': (1-allLipsPredictions[2]).flatten().tolist(),\n",
    "        'LipFSAvg': 1-np.average(allLipsPredictions[2]),\n",
    "        'LipNT': (1-allLipsPredictions[3]).flatten().tolist(),\n",
    "        'LipNTAvg': 1-np.average(allLipsPredictions[3]),\n",
    "        'FDA_DF': fdaPredictions['deepfake'],\n",
    "        'FDA_F2F': fdaPredictions['face2face'],\n",
    "        'FDA_FS': fdaPredictions['faceswap'],\n",
    "        'FDA_NT': fdaPredictions['neuraltextures'],\n",
    "        'DTBinaryDF': (dynamicTexturePredictionsBinary[0]).flatten().tolist(),\n",
    "        'DTBinaryDFAvg': np.average(dynamicTexturePredictionsBinary[0]),\n",
    "        'DTBinaryF2F': (dynamicTexturePredictionsBinary[1]).flatten().tolist(),\n",
    "        'DTBinaryF2FAvg': np.average(dynamicTexturePredictionsBinary[1]),\n",
    "        'DTBinaryFS': (dynamicTexturePredictionsBinary[2]).flatten().tolist(),\n",
    "        'DTBinaryFSAvg': np.average(dynamicTexturePredictionsBinary[2]),\n",
    "        'DTBinaryNT': (dynamicTexturePredictionsBinary[3]).flatten().tolist(),\n",
    "        'DTBinaryNTAvg': np.average(dynamicTexturePredictionsBinary[3]),\n",
    "        \n",
    "        'DTMulti': (dynamicTexturePredictionsMulti).flatten().tolist(),\n",
    "\n",
    "        'SBI': sbiPredictions,\n",
    "        'SBIAvg': np.average(sbiPredictions)   \n",
    "    }\n",
    "    return resultmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_video(videoCapture):\n",
    "    #read video frames\n",
    "    print('Reading video frames...')\n",
    "    grayFrames, coloredFrames = read_video_frames(videoCapture)\n",
    "\n",
    "    print('Detecting face region...')\n",
    "    #initialize face detector \n",
    "    face_detector = initialize_face_detector()\n",
    "    #detect and crop face region\n",
    "    faceFrames, coloredFaceFrames = get_face_frames(face_detector, grayFrames, coloredFrames)\n",
    "    \n",
    "    print('Detecting landmarks...')\n",
    "    #detect landmarks\n",
    "    landmarksDetector = LandmarksDetector(isPredictor=True, modelspath='../../Facial-Landmarks-Detector/landmarksmodels')\n",
    "    framesLandmarks = []\n",
    "    for frame in faceFrames:\n",
    "        framesLandmarks.append(landmarksDetector.predict(frame, (0, 0, frame.shape[1], frame.shape[0])))\n",
    "    framesLandmarks = np.array(framesLandmarks)\n",
    "    \n",
    "    print('Lips Movement Analysis...')\n",
    "    #Lips Movement Analysis \n",
    "    lipsMovementModel = initializeLipsMovementModel()\n",
    "    allLipsPredictions = lipsMovementModel.predict(faceFrames, framesLandmarks)\n",
    "\n",
    "    print('Frequency Domain Analysis...')\n",
    "    #Frequency Domain Analysis\n",
    "    fdaModel = FDA(model_path='../../Modules/dft/Models')\n",
    "    fdaPredictions = fdaModel.predict(faceFrames)\n",
    "\n",
    "    print('Dynamic Texture Analysis...')\n",
    "    #Dynamic Texture Analysis\n",
    "    dynamicTextureObjectBinary = dynamicTexture('cf23', 'binary', '../../Dynamic-texture-analysis-for-detecting-fake-faces-in-video-sequences/models/')\n",
    "    dynamicTextureObjectMulti = dynamicTexture('cf23', 'multi', '../../Dynamic-texture-analysis-for-detecting-fake-faces-in-video-sequences/models/')\n",
    "    dynamicTexturePredictionsBinary = dynamicTextureObjectBinary.predict(faceFrames, 30)\n",
    "    dynamicTexturePredictionsMulti = dynamicTextureObjectMulti.predict(faceFrames, 30)\n",
    "\n",
    "    print('SBI Analysis...')\n",
    "    #SBI Analysis\n",
    "    sbimodel = SBI_inference('../../SBI2/36_0.9899_val.tar')\n",
    "    sbiPredictions = sbi_analysis(sbimodel, 32, coloredFaceFrames)\n",
    "\n",
    "    return extract_analysis(allLipsPredictions, fdaPredictions, dynamicTexturePredictionsBinary, dynamicTexturePredictionsMulti, sbiPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ammar\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "from revelio import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading video frames...\n",
      "Detecting face region...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ammar\\anaconda3\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting landmarks...\n",
      "Lips Movement Analysis...\n",
      "Frequency Domain Analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ammar\\anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator SVC from version 1.2.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic Texture Analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ammar\\anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator SVC from version 1.2.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBI Analysis...\n",
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'LipDF': [0.9024010300636292,\n",
       "  0.8124421238899231,\n",
       "  0.16847741603851318,\n",
       "  1.0,\n",
       "  0.999993622303009,\n",
       "  1.0,\n",
       "  0.9999570250511169,\n",
       "  0.9999986886978149,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9999997615814209],\n",
       " 'LipDFAvg': 0.9069391414523125,\n",
       " 'LipF2F': [0.05867832899093628,\n",
       "  0.48605281114578247,\n",
       "  0.0030385851860046387,\n",
       "  0.025774776935577393,\n",
       "  0.0006885528564453125,\n",
       "  1.1920928955078125e-07,\n",
       "  0.2463863492012024,\n",
       "  0.8766398429870605,\n",
       "  0.00032460689544677734,\n",
       "  0.9371023178100586,\n",
       "  0.7798152565956116,\n",
       "  2.384185791015625e-07],\n",
       " 'LipF2FAvg': 0.2845418453216553,\n",
       " 'LipFS': [0.013775825500488281,\n",
       "  0.9344635605812073,\n",
       "  0.35719382762908936,\n",
       "  0.018836677074432373,\n",
       "  1.430511474609375e-06,\n",
       "  0.45079827308654785,\n",
       "  0.0013275146484375,\n",
       "  0.0014240741729736328,\n",
       "  0.011612355709075928,\n",
       "  0.12604427337646484,\n",
       "  0.6674356460571289,\n",
       "  0.8218920230865479],\n",
       " 'LipFSAvg': 0.2837337851524353,\n",
       " 'LipNT': [0.9640939235687256,\n",
       "  0.9168065190315247,\n",
       "  0.9997347593307495,\n",
       "  0.9965834617614746,\n",
       "  0.1146964430809021,\n",
       "  0.7322514057159424,\n",
       "  0.05447232723236084,\n",
       "  0.9359201788902283,\n",
       "  0.0008448362350463867,\n",
       "  0.22584408521652222,\n",
       "  1.4662742614746094e-05,\n",
       "  1.3113021850585938e-05],\n",
       " 'LipNTAvg': 0.4951062798500061,\n",
       " 'FDA_DF': 0,\n",
       " 'FDA_F2F': 0,\n",
       " 'FDA_FS': 0,\n",
       " 'FDA_NT': 0,\n",
       " 'DTBinaryDF': [1, 1, 0, 0],\n",
       " 'DTBinaryDFAvg': 0.5,\n",
       " 'DTBinaryF2F': [1, 0, 0, 0],\n",
       " 'DTBinaryF2FAvg': 0.25,\n",
       " 'DTBinaryFS': [0, 0, 0, 0],\n",
       " 'DTBinaryFSAvg': 0.0,\n",
       " 'DTBinaryNT': [1, 0, 0, 0],\n",
       " 'DTBinaryNTAvg': 0.25,\n",
       " 'DTMulti': [1, 1, 0, 0],\n",
       " 'SBI': [0.31752777,\n",
       "  0.4154943,\n",
       "  0.36306804,\n",
       "  0.35060728,\n",
       "  0.42620042,\n",
       "  0.50252897,\n",
       "  0.53119284,\n",
       "  0.53473127,\n",
       "  0.3474447],\n",
       " 'SBIAvg': 0.4209773}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revelio = Revelio()\n",
    "revelio.analyze_video(cv2.VideoCapture('002_006.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysisResult = analyze_video(cv2.VideoCapture('002_006.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sbiPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultMap = extract_analysis(allLipsPredictions, fdaPredictions, dynamicTexturePredictionsBinary,dynamicTexturePredictionsMulti, sbiPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdaPredictions['deepfake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(allLipsPredictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read video\n",
    "cap = cv2.VideoCapture('002_006.mp4')\n",
    "\n",
    "#read frames into numpy array\n",
    "grayFrames = []\n",
    "coloredFrames = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        #frame to gray scale\n",
    "        coloredFrames.append(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        grayFrames.append(frame)\n",
    "    else:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayFrames[0].shape[1]//4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img =cv2.resize(grayFrames[0], (grayFrames[0].shape[1], grayFrames[0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot rectangle on img\n",
    "cv2.rectangle(img, (region[0], region[1]), (region[2], region[3]), (255, 255, 255), 2)\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop face from frames\n",
    "faceFrames = []\n",
    "coloredFaceFrames = []\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "for i in range(len(grayFrames)):\n",
    "    frame = grayFrames[i]\n",
    "    coloredFrame = coloredFrames[i]\n",
    "    face = detector(frame)[0]\n",
    "    x1 = face.left()\n",
    "    y1 = face.top()\n",
    "    x2 = face.right()\n",
    "    y2 = face.bottom()\n",
    "    faceFrames.append(frame[y1:y2, x1:x2])\n",
    "    coloredFaceFrames.append(coloredFrame[y1:y2, x1:x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('coloredFaceFrames006.npz', frames=coloredFaceFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faceFrames = np.array(faceFrames)\n",
    "# framesLandmarks = np.array(framesLandmarks)\n",
    "faceFrames = np.load('faceFrames.npz')['frames']\n",
    "framesLandmarks = np.load('framesLandmarks.npz')['landmarks']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipsMovementModel = initializeLipsMovementModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lipsMovementModel.predict(faceFrames, framesLandmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(faceFrames.shape)\n",
    "print(framesLandmarks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coloredFaceFrames = np.load('coloredFaceFrames006.npz')['frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coloredFaceFrames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitFrames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbi_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
